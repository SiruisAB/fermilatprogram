import os 
import numpy as np
from fermipy.gtanalysis import GTAnalysis
import astropy.io.fits as pyfits
import matplotlib.pyplot as plt
import pandas as pd
import threading
import concurrent.futures
from queue import Queue
import time
from datetime import datetime
import logging
import argparse
import sys
from astropy.coordinates import SkyCoord
import astropy.units as u
from Generate_gconfig import parse_grb_info as parse_grb_info_from_module
# å¯¼å…¥photon_analyzeræ¨¡å—ä¸­çš„å‡½æ•°
from photon_analyzer import find_highest_prob_photon, save_all_photons
from cleandir import clean_results_directory
from Generate_gconfig import create_config
# é…ç½®å‚æ•°
BASE_DIR = "/home/mxr/lee/data/fermilat"
TEMPLATE_CONFIG = "/home/mxr/lee/config.yaml"  # æ ‡å‡†é…ç½®æ–‡ä»¶æ¨¡æ¿
GRB_DATA_DIR = os.path.join(BASE_DIR, "grb_data")
RESULTS_DIR = os.path.join(BASE_DIR, "resultsPL")

# å¤šçº¿ç¨‹é…ç½®
MAX_WORKERS = 4  # æœ€å¤§å¹¶è¡Œçº¿ç¨‹æ•°
THREAD_TIMEOUT = 3600  # å•ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# è®¾ç½®å¤šçº¿ç¨‹æ—¥å¿—
def setup_logging():
    """è®¾ç½®å¤šçº¿ç¨‹å®‰å…¨çš„æ—¥å¿—ç³»ç»Ÿ"""
    log_format = '%(asctime)s - %(threadName)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(os.path.join(RESULTS_DIR, 'analysis.log')),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# çº¿ç¨‹å®‰å…¨çš„ç»“æœæ”¶é›†å™¨
class ResultCollector:
    def __init__(self):
        self._lock = threading.Lock()
        self._results = {}
        self._errors = {}
    
    def add_result(self, grb_name, result):
        with self._lock:
            self._results[grb_name] = result
            logger.info(f"âœ… {grb_name} åˆ†æå®Œæˆ")
    
    def add_error(self, grb_name, error):
        with self._lock:
            self._errors[grb_name] = str(error)
            logger.error(f"âŒ {grb_name} åˆ†æå¤±è´¥: {error}")
    
    def get_results(self):
        with self._lock:
            return self._results.copy(), self._errors.copy()
    
    def get_summary(self):
        with self._lock:
            return len(self._results), len(self._errors)

def parse_grb_info(grb_dir):
    """è§£æGRBä¿¡æ¯æ–‡ä»¶ï¼ˆè°ƒç”¨Generate_gconfigæ¨¡å—ï¼‰"""
    grb_name = os.path.basename(grb_dir)
    return parse_grb_info_from_module(grb_name, RESULTS_DIR)

def analyze_grb_worker(grb_name, result_collector):
    """æ‰§è¡Œå•ä¸ªGRBçš„åˆ†ææµç¨‹ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
    thread_id = threading.current_thread().name
    start_time = time.time()
    
    try:
        logger.info(f"[{thread_id}] {'='*40}")
        logger.info(f"[{thread_id}] å¼€å§‹åˆ†æ: {grb_name}")
        logger.info(f"[{thread_id}] {'='*40}")
        
        grb_params = parse_grb_info(os.path.join(GRB_DATA_DIR, grb_name))

        config_path = os.path.join(RESULTS_DIR, grb_name, "config.yaml")
        # 3. è®¾ç½®åˆ†æç¯å¢ƒ
        try:
            gta = GTAnalysis(config_path, logging={'verbosity': 1})  # é™ä½æ—¥å¿—çº§åˆ«é¿å…å†²çª
            gta.setup()
            logger.info(f"[{thread_id}] {grb_name} FermiPy ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise Exception(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # 5. æ‰“å°åˆå§‹ROIæ¨¡å‹
        try:
            gta.print_roi()
            logger.info(f"[{thread_id}] {grb_name} ROIæ¨¡å‹æ‰“å°å®Œæˆ")
        except Exception as e:
            logger.warning(f"[{thread_id}] {grb_name} æ‰“å°ROIå¤±è´¥: {str(e)}")
        
        # 6. è®¾ç½®æ‹Ÿåˆå‚æ•°
        try:
            # é‡Šæ”¾é™„è¿‘æº
            gta.optimize()
            gta.free_sources(distance=2.5, pars='norm')
            
            # é‡Šæ”¾å¼¥æ•£èƒŒæ™¯
            gta.free_source('galdiff')
            gta.free_source('isodiff')
            
            # é‡Šæ”¾ç›®æ ‡GRB
            gta.free_source(grb_name)
            logger.info(f"[{thread_id}] {grb_name} æ‹Ÿåˆå‚æ•°è®¾ç½®å®Œæˆ")
        except Exception as e:
            logger.warning(f"[{thread_id}] {grb_name} è®¾ç½®æ‹Ÿåˆå‚æ•°å¤±è´¥: {str(e)}")
        
        # 7. æ‰§è¡Œæ‹Ÿåˆ
        try:
            fit_results = gta.fit()
            logger.info(f"[{thread_id}] {grb_name} æ‹Ÿåˆç»“æœ:")
            logger.info(f"[{thread_id}] {grb_name} Fit Quality: {fit_results['fit_quality']}")
            logger.info(f"[{thread_id}] {grb_name} ç›®æ ‡æºä¿¡æ¯: {gta.roi[grb_name]}")
            
        except Exception as e:
            raise Exception(f"æ‹Ÿåˆå¤±è´¥: {str(e)}")
        # 8. TSMAP and residmap
        output_base = os.path.join(RESULTS_DIR, grb_name, "fit0")
        gta.write_roi(output_base, make_plots=True)
        fit_npy_path = os.path.join(RESULTS_DIR, grb_name, "fit0.npy")
        try:
            c = np.load(fit_npy_path, allow_pickle=True).flat[0]
        except Exception as e:
            logger.warning(f"[{thread_id}] {grb_name} åŠ è½½æ‹Ÿåˆç»“æœnpyå¤±è´¥: {str(e)}")

        # 8. æ‰§è¡ŒSEDåˆ†æ
        try:
            from sed_plotter import plot_sed, save_sed_plot
            # ç»˜åˆ¶SEDå›¾åƒ
            sed = gta.sed(f'{grb_name}',loge_bins=np.linspace(2, 5, num=6),use_local_index=True)
            plot_sed(c, sed, f'{grb_name}')
            # ä¿å­˜å›¾åƒåˆ°å„ä¸ªGRBåˆ†ææ–‡ä»¶å¤¹
            grb_result_dir = os.path.join(RESULTS_DIR, grb_name)
            sed_image_path = os.path.join(grb_result_dir, f'{grb_name}_sed.png')
            save_sed_plot(c, sed, f'{grb_name}', sed_image_path)
        except Exception as e:
            logger.warning(f"[{thread_id}] {grb_name} SEDåˆ†æå¤±è´¥: {str(e)}")

        # 9. è·å–æœ€é«˜èƒ½é«˜æ¦‚ç‡å…‰å­ä¿¡æ¯ï¼ˆåœ¨æ‹Ÿåˆåè¿›è¡Œï¼‰
        highest_photon = find_highest_prob_photon(gta, grb_name, grb_params, RESULTS_DIR)
        # 10. ä¿å­˜æ‹Ÿåˆç»“æœï¼ˆåŒ…å«æœ€é«˜èƒ½å…‰å­ä¿¡æ¯å’Œåˆ†ææ‘˜è¦ï¼‰
        if fit_results:
            try:
                result_path = os.path.join(RESULTS_DIR, grb_name, f"{grb_name}_fit_results.txt")
                analysis_time = time.time() - start_time
                
                with open(result_path, 'w') as f:
                    f.write(f"GRB Analysis Results for {grb_name}\n")
                    f.write("="*50 + "\n")
                    f.write(f"åˆ†æç›®å½•: {BASE_DIR}\n")
                    f.write(f"åˆ†ææ—¶é—´: {pd.Timestamp.now()}\n")
                    f.write(f"åˆ†æè€—æ—¶: {analysis_time:.2f}s\n")
                    f.write("\n")
                    
                    # æ·»åŠ GRBåŸºæœ¬å‚æ•°ä¿¡æ¯ï¼ˆåŒ…å«T0, T1ï¼‰
                    f.write("GRB Parameters:\n")
                    f.write("-"*20 + "\n")
                    f.write(f"RA: {grb_params['ra']:.4f} deg\n")
                    f.write(f"Dec: {grb_params['dec']:.4f} deg\n")
                    f.write(f"Trigger MET: {grb_params['trigger_met']:.2f} s\n")
                    f.write(f"T0: {grb_params['T0']:.2f} s\n")
                    f.write(f"T1: {grb_params['T1']:.2f} s\n")
                    f.write(f"Time Range (tmin-tmax): {grb_params['tmin']:.2f} - {grb_params['tmax']:.2f} s\n")
                    if 'PIndex' in grb_params:
                        f.write(f"PIndex: {grb_params['PIndex']}\n")
                    f.write("\n")
                    
                    # æ·»åŠ æ‹Ÿåˆç»“æœä¿¡æ¯
                    f.write("Fit Results:\n")
                    f.write("-"*20 + "\n")
                    f.write(f"Fit Quality: {fit_results['fit_quality']}\n")
                    f.write(f"Log-Likelihood: {fit_results['loglike']:.2f}\n")
                    f.write(f"Target Source: {gta.roi[grb_name]}\n\n")
                                       
                    # æ·»åŠ æœ€é«˜èƒ½é«˜æ¦‚ç‡å…‰å­ä¿¡æ¯
                    f.write("Highest Energy Photon Analysis:\n")
                    f.write("-"*30 + "\n")
                    if highest_photon:
                        f.write("Highest Energy Photon (Probability > 0.9):\n")
                        f.write(f"  Energy: {highest_photon['energy']:.2f} MeV\n")
                        f.write(f"  Probability: {highest_photon['probability']:.4f}\n")
                        f.write(f"  Time (MET): {highest_photon['time']:.2f} s\n")
                        f.write(f"  Relative Time: {highest_photon['relative_time']:.2f} s\n")
                        f.write(f"  RA: {highest_photon['ra']:.4f} deg\n")
                        f.write(f"  Dec: {highest_photon['dec']:.4f} deg\n")
                        f.write(f"  Angular Separation: {highest_photon['angular_separation']:.4f} deg\n")
                        f.write(f"  Total High Prob Photons: {highest_photon['total_high_prob_photons']}\n")
                        f.write(f"  Event Class: {highest_photon['event_class']}\n")
                        f.write(f"  Event Type: {highest_photon['event_type']}\n")
                    else:
                        f.write("Highest Energy Photon: Not found with probability > 0.9\n")
                    
                    f.write("\n")
                    f.write("="*50 + "\n")
                    f.write(f"Analysis completed at: {pd.Timestamp.now()}\n")
                
                logger.info(f"[{thread_id}] {grb_name} æ‹Ÿåˆç»“æœå·²ä¿å­˜: {result_path}")
                
                # å°†æœ€é«˜èƒ½å…‰å­ä¿¡æ¯æ·»åŠ åˆ°è¿”å›ç»“æœä¸­
                if highest_photon:
                    fit_results['highest_photon'] = highest_photon
                    
            except Exception as e:
                logger.warning(f"[{thread_id}] {grb_name} ä¿å­˜æ‹Ÿåˆç»“æœå¤±è´¥: {str(e)}")
        
        # 11. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œå›¾è¡¨
        try:
            output_base = os.path.join(RESULTS_DIR, grb_name, "final_model")
            gta.write_roi(output_base, make_plots=True)
            logger.info(f"[{thread_id}] {grb_name} æœ€ç»ˆæ¨¡å‹å’Œå›¾è¡¨å·²ä¿å­˜: {output_base}.*")
        except Exception as e:
            logger.warning(f"[{thread_id}] {grb_name} ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {str(e)}")
        
        # æ”¶é›†ç»“æœ
        if fit_results:
            analysis_result = {
                'fit_results': fit_results,
                'grb_params': grb_params,
                'analysis_time': time.time() - start_time
            }
            result_collector.add_result(grb_name, analysis_result)
        else:
            result_collector.add_error(grb_name, "æ‹Ÿåˆå¤±è´¥")
    
    except Exception as e:
        result_collector.add_error(grb_name, str(e))
        logger.error(f"[{thread_id}] {grb_name} åˆ†æå¼‚å¸¸: {str(e)}")

def get_grb_list():
    """è·å–å¾…åˆ†æçš„GRBåˆ—è¡¨"""
    grb_dirs = [d for d in os.listdir(GRB_DATA_DIR) 
                if os.path.isdir(os.path.join(GRB_DATA_DIR, d))
                and d.startswith('GRB')]
    return grb_dirs

def analyze_grb_multithread(grb_list=None, max_workers=MAX_WORKERS):
    """å¤šçº¿ç¨‹åˆ†æå¤šä¸ªGRB"""
    
    if grb_list is None:
        grb_list = get_grb_list()
    
    if not grb_list:
        logger.warning("æœªæ‰¾åˆ°å¾…åˆ†æçš„GRBæ–‡ä»¶")
        return None, None
    
    logger.info(f"ğŸ¯ å‡†å¤‡åˆ†æ {len(grb_list)} ä¸ªGRBæ–‡ä»¶")
    logger.info(f"ğŸ“‹ GRBåˆ—è¡¨: {', '.join(grb_list)}")
    logger.info(f"ğŸ”§ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")
    
    result_collector = ResultCollector()
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers, 
                                               thread_name_prefix="GRB-Worker") as executor:
        
        future_to_grb = {}
        for grb_name in grb_list:
            future = executor.submit(analyze_grb_worker, grb_name, result_collector)
            future_to_grb[future] = grb_name
        
        completed = 0
        total = len(grb_list)
        
        for future in concurrent.futures.as_completed(future_to_grb, timeout=THREAD_TIMEOUT):
            grb_name = future_to_grb[future]
            completed += 1
            
            try:
                future.result()
                success_count, error_count = result_collector.get_summary()
                logger.info(f"ğŸ“Š è¿›åº¦: {completed}/{total} | æˆåŠŸ: {success_count} | å¤±è´¥: {error_count}")
                
            except concurrent.futures.TimeoutError:
                logger.error(f"â° {grb_name} åˆ†æè¶…æ—¶")
                result_collector.add_error(grb_name, "åˆ†æè¶…æ—¶")
            except Exception as e:
                logger.error(f"ğŸ’¥ {grb_name} çº¿ç¨‹å¼‚å¸¸: {str(e)}")
    
    results, errors = result_collector.get_results()
    total_time = time.time() - start_time
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ‰ å¤šçº¿ç¨‹åˆ†æå®Œæˆ!")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"âœ… æˆåŠŸ: {len(results)} ä¸ª")
    logger.info(f"âŒ å¤±è´¥: {len(errors)} ä¸ª")
    
    if errors:
        logger.info(f"\nå¤±è´¥çš„GRB:")
        for grb_name, error in errors.items():
            logger.info(f"  - {grb_name}: {error}")
    
    logger.info(f"{'='*60}")
    
    return results, errors

# =====================
# ä¸»ç¨‹åº
# =====================

def analyze_single_grb(grb_name):
    """åˆ†æå•ä¸ªæŒ‡å®šçš„GRBäº‹ä»¶"""
    logger.info(f"ğŸ¯ å¼€å§‹åˆ†æå•ä¸ªGRB: {grb_name}")
    
    # æ£€æŸ¥GRBæ˜¯å¦å­˜åœ¨
    grb_path = os.path.join(GRB_DATA_DIR, grb_name)
    if not os.path.exists(grb_path):
        logger.error(f"âŒ GRBç›®å½•ä¸å­˜åœ¨: {grb_path}")
        available_grbs = get_grb_list()
        if available_grbs:
            logger.info(f"ğŸ“‹ å¯ç”¨çš„GRBåˆ—è¡¨: {', '.join(available_grbs)}")
        return None, {grb_name: "GRBç›®å½•ä¸å­˜åœ¨"}
    
    # åˆ›å»ºç»“æœæ”¶é›†å™¨
    result_collector = ResultCollector()
    start_time = time.time()
    
    # æ‰§è¡Œåˆ†æ
    analyze_grb_worker(grb_name, result_collector)
    
    # è·å–ç»“æœ
    results, errors = result_collector.get_results()
    total_time = time.time() - start_time
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ‰ å•ä¸ªGRBåˆ†æå®Œæˆ!")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    if results:
        logger.info(f"âœ… åˆ†ææˆåŠŸ: {grb_name}")
    if errors:
        logger.info(f"âŒ åˆ†æå¤±è´¥: {grb_name} - {errors[grb_name]}")
    logger.info(f"{'='*60}")
    
    return results, errors

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='GRBæ•°æ®åˆ†æå·¥å…· - æ”¯æŒå¤šçº¿ç¨‹æ‰¹é‡åˆ†ææˆ–å•ä¸ªGRBåˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ä½¿ç”¨ç¤ºä¾‹:
  python lkmulty.py                    # åˆ†ææ‰€æœ‰GRBï¼ˆå¤šçº¿ç¨‹ï¼‰
  python lkmulty.py --grb GRB090510    # åˆ†ææŒ‡å®šçš„å•ä¸ªGRB
  python lkmulty.py --list             # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„GRB
  python lkmulty.py --workers 8        # ä½¿ç”¨8ä¸ªçº¿ç¨‹è¿›è¡Œæ‰¹é‡åˆ†æ"""
    )
    
    parser.add_argument(
        '--grb', 
        type=str, 
        help='æŒ‡å®šè¦åˆ†æçš„å•ä¸ªGRBäº‹ä»¶åç§°ï¼ˆä¾‹å¦‚ï¼šGRB090510ï¼‰'
    )
    
    parser.add_argument(
        '--list', 
        action='store_true', 
        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„GRBäº‹ä»¶'
    )
    
    parser.add_argument(
        '--workers', 
        type=int, 
        default=MAX_WORKERS,
        help=f'å¤šçº¿ç¨‹åˆ†ææ—¶çš„çº¿ç¨‹æ•°é‡ï¼ˆé»˜è®¤ï¼š{MAX_WORKERS}ï¼‰'
    )
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    clean_results_directory(target_dir=RESULTS_DIR)
    grb_name = args.grb
    if grb_name:
        grb_params = parse_grb_info_from_module(grb_name)
        output_dir = os.path.join(RESULTS_DIR, grb_name)
        create_config(grb_name, grb_params, output_dir=output_dir)

    try:
        # å¦‚æœè¯·æ±‚åˆ—å‡ºGRBåˆ—è¡¨
        if args.list:
            grb_list = get_grb_list()
            if grb_list:
                logger.info(f"ğŸ“‹ å‘ç° {len(grb_list)} ä¸ªå¯ç”¨çš„GRBäº‹ä»¶:")
                for i, grb in enumerate(sorted(grb_list), 1):
                    logger.info(f"  {i:2d}. {grb}")
            else:
                logger.warning(f"âŒ åœ¨ç›®å½• {GRB_DATA_DIR} ä¸­æœªæ‰¾åˆ°ä»»ä½•GRBæ•°æ®")
            return
        
        # å¦‚æœæŒ‡å®šäº†å•ä¸ªGRB
        if args.grb:
            results, errors = analyze_single_grb(args.grb)
        else:
            # å¼€å§‹å¤šçº¿ç¨‹åˆ†ææ‰€æœ‰GRB
            logger.info(f"ğŸ”§ ä½¿ç”¨ {args.workers} ä¸ªçº¿ç¨‹è¿›è¡Œæ‰¹é‡åˆ†æ")
            results, errors = analyze_grb_multithread(max_workers=args.workers)
        
        if results:
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆä½¿ç”¨æ–°çš„ç»“æœæ ¼å¼ï¼‰
            if args.grb:
                # å•ä¸ªGRBåˆ†ææŠ¥å‘Š
                summary_path = os.path.join(RESULTS_DIR, f"{args.grb}_analysis_summary.txt")
                report_title = f"GRBå•ä¸ªäº‹ä»¶åˆ†ææŠ¥å‘Š: {args.grb}"
            else:
                # å¤šçº¿ç¨‹æ‰¹é‡åˆ†ææŠ¥å‘Š
                summary_path = os.path.join(RESULTS_DIR, "analysis_summary.txt")
                report_title = "GRBå¤šçº¿ç¨‹åˆ†ææ±‡æ€»æŠ¥å‘Š"
            
            with open(summary_path, 'w') as f:
                f.write(f"{report_title}\n")
                f.write("="*50 + "\n")
                f.write(f"åˆ†æç›®å½•: {BASE_DIR}\n")
                f.write(f"åˆ†ææ—¶é—´: {pd.Timestamp.now()}\n")
                f.write(f"æˆåŠŸåˆ†æ: {len(results)} ä¸ªGRB\n")
                f.write(f"å¤±è´¥åˆ†æ: {len(errors)} ä¸ªGRB\n\n")
                
                for grb, result_data in results.items():
                    fit_results = result_data['fit_results']
                    grb_params = result_data['grb_params']
                    f.write(f"{grb}:\n")
                    f.write("="*30 + "\n")
                    f.write(f"  åˆ†ææ—¶é—´: {result_data['analysis_time']:.2f}s\n")
                    f.write(f"  Fit Quality: {fit_results.get('fit_quality', 'N/A')}\n")
                    f.write(f"  Log-Likelihood: {fit_results.get('loglike', 'N/A')}\n")
                    
                    # æ·»åŠ GRBåŸºæœ¬å‚æ•°ä¿¡æ¯
                    f.write("  GRBå‚æ•°:\n")
                    f.write(f"    RA: {grb_params['ra']:.4f} deg\n")
                    f.write(f"    Dec: {grb_params['dec']:.4f} deg\n")
                    f.write(f"    Trigger MET: {grb_params['trigger_met']:.2f} s\n")
                    f.write(f"    T0: {grb_params['T0']:.2f} s\n")
                    f.write(f"    T1: {grb_params['T1']:.2f} s\n")
                    f.write(f"    Time Range: {grb_params['tmin']:.2f} - {grb_params['tmax']:.2f} s\n")
                    if 'PIndex' in grb_params:
                        f.write(f"    PIndex: {grb_params['PIndex']}\n")
                                     
                    if 'highest_photon' in fit_results and fit_results['highest_photon']:
                        hp = fit_results['highest_photon']
                        f.write("  æœ€é«˜èƒ½å…‰å­ä¿¡æ¯:\n")
                        f.write(f"    èƒ½é‡: {hp['energy']:.2f} MeV\n")
                        f.write(f"    æ¦‚ç‡: {hp['probability']:.4f}\n")
                        f.write(f"    ç›¸å¯¹æ—¶é—´: {hp['relative_time']:.2f} s\n")
                        f.write(f"    RA: {hp['ra']:.4f} deg\n")
                        f.write(f"    Dec: {hp['dec']:.4f} deg\n")
                        f.write(f"    è§’åˆ†ç¦»: {hp['angular_separation']:.4f} deg\n")
                        f.write(f"    é«˜æ¦‚ç‡å…‰å­æ€»æ•°: {hp['total_high_prob_photons']}\n")
                        f.write(f"    äº‹ä»¶ç±»å‹: {hp['event_class']}, {hp['event_type']}\n")
                    else:
                        f.write("  æœ€é«˜èƒ½å…‰å­: æœªæ‰¾åˆ°æ¦‚ç‡>0.9çš„å…‰å­\n")
                    
                    # å°è¯•ä»è¯¦ç»†ç»“æœæ–‡ä»¶ä¸­è¯»å–æ›´å¤šä¿¡æ¯
                    try:
                        result_file_path = os.path.join(RESULTS_DIR, grb, f"{grb}_fit_results.txt")
                        if os.path.exists(result_file_path):
                            f.write("  è¯¦ç»†åˆ†æç»“æœ:\n")
                            with open(result_file_path, 'r') as detail_file:
                                lines = detail_file.readlines()
                                # æŸ¥æ‰¾ç›®æ ‡æºä¿¡æ¯
                                for i, line in enumerate(lines):
                                    if "Target Source:" in line:
                                        f.write(f"    ç›®æ ‡æºä¿¡æ¯: {line.split('Target Source:')[1].strip()}\n")
                                        break
                    except Exception as e:
                        f.write(f"  è¯¦ç»†ä¿¡æ¯è¯»å–å¤±è´¥: {str(e)}\n")
                    
                    f.write("\n")
                
                if errors:
                    f.write("å¤±è´¥çš„GRB:\n")
                    for grb, error in errors.items():
                        f.write(f"  {grb}: {error}\n")
            
            logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
            
            # ç”Ÿæˆå…‰å­ä¿¡æ¯çš„æ±‡æ€»æŠ¥å‘Š
            try:
                all_photons_summary = []
                for grb, result_data in results.items():
                    grb_dir = os.path.join(RESULTS_DIR, grb)
                    all_photons_file = os.path.join(grb_dir, f"{grb}_all_photons.csv")
                    if os.path.exists(all_photons_file):
                        df = pd.read_csv(all_photons_file)
                        df['GRB'] = grb  # æ·»åŠ GRBåç§°åˆ—
                        all_photons_summary.append(df)
                
                if all_photons_summary:
                    # åˆå¹¶æ‰€æœ‰GRBçš„å…‰å­ä¿¡æ¯
                    combined_df = pd.concat(all_photons_summary, ignore_index=True)
                    
                    if args.grb:
                        # å•ä¸ªGRBçš„å…‰å­ä¿¡æ¯
                        combined_path = os.path.join(RESULTS_DIR, f"{args.grb}_photons.csv")
                        logger.info(f"å•ä¸ªGRBçš„å…‰å­ä¿¡æ¯å·²ä¿å­˜: {combined_path}")
                    else:
                        # æ‰€æœ‰GRBçš„å…‰å­ä¿¡æ¯æ±‡æ€»
                        combined_path = os.path.join(RESULTS_DIR, "all_grbs_photons.csv")
                        logger.info(f"æ‰€æœ‰GRBçš„å…‰å­ä¿¡æ¯æ±‡æ€»å·²ä¿å­˜: {combined_path}")
                    
                    combined_df.to_csv(combined_path, index=False)
            except Exception as e:
                logger.error(f"ç”Ÿæˆå…‰å­ä¿¡æ¯æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {str(e)}")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()